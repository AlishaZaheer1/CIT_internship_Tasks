{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4V3obc9Up8cFSA/iY6TqP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlishaZaheer1/CIT_internship_Tasks/blob/main/Untitled109.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BREAST CANCER DETECTION TASK"
      ],
      "metadata": {
        "id": "UK6icjhh9K30"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "V-jDTQBD8E_1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from URL\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
        "columns = [\n",
        "    'ID', 'Diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
        "    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',\n",
        "    'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se',\n",
        "    'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave_points_se',\n",
        "    'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst',\n",
        "    'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst',\n",
        "    'concave_points_worst', 'symmetry_worst', 'fractal_dimension_worst'\n",
        "]\n",
        "data = pd.read_csv(url, header=None, names=columns)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgjdAQ8Y8Oqe",
        "outputId": "fb769e70-52af-46a2-aa8f-727262268ee0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         ID Diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0    842302         M        17.99         10.38          122.80     1001.0   \n",
            "1    842517         M        20.57         17.77          132.90     1326.0   \n",
            "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
            "3  84348301         M        11.42         20.38           77.58      386.1   \n",
            "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
            "\n",
            "   smoothness_mean  compactness_mean  concavity_mean  concave_points_mean  \\\n",
            "0          0.11840           0.27760          0.3001              0.14710   \n",
            "1          0.08474           0.07864          0.0869              0.07017   \n",
            "2          0.10960           0.15990          0.1974              0.12790   \n",
            "3          0.14250           0.28390          0.2414              0.10520   \n",
            "4          0.10030           0.13280          0.1980              0.10430   \n",
            "\n",
            "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
            "0  ...         25.38          17.33           184.60      2019.0   \n",
            "1  ...         24.99          23.41           158.80      1956.0   \n",
            "2  ...         23.57          25.53           152.50      1709.0   \n",
            "3  ...         14.91          26.50            98.87       567.7   \n",
            "4  ...         22.54          16.67           152.20      1575.0   \n",
            "\n",
            "   smoothness_worst  compactness_worst  concavity_worst  concave_points_worst  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   symmetry_worst  fractal_dimension_worst  \n",
            "0          0.4601                  0.11890  \n",
            "1          0.2750                  0.08902  \n",
            "2          0.3613                  0.08758  \n",
            "3          0.6638                  0.17300  \n",
            "4          0.2364                  0.07678  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# Convert 'Diagnosis' column to binary\n",
        "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n",
        "\n",
        "# Drop the 'ID' column as it is not useful for prediction\n",
        "data.drop('ID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "B-xdWBtI8rVZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target (y)\n",
        "X = data.drop(['Diagnosis'], axis=1)\n",
        "y = data['Diagnosis']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "_7k5oFqU8v8D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"KNN Classifier\")\n",
        "print(confusion_matrix(y_test, y_pred_knn))\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjQqgHIj8yCu",
        "outputId": "ab166603-4451-4310-c2fa-f5b9a072c116"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classifier\n",
            "[[68  3]\n",
            " [ 3 40]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.96      0.96        71\n",
            "           1       0.93      0.93      0.93        43\n",
            "\n",
            "    accuracy                           0.95       114\n",
            "   macro avg       0.94      0.94      0.94       114\n",
            "weighted avg       0.95      0.95      0.95       114\n",
            "\n",
            "Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HOUSE PREDICTION** **TASK**"
      ],
      "metadata": {
        "id": "Je69BKQ78-W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import requests\n",
        "from io import StringIO\n"
      ],
      "metadata": {
        "id": "LHSO3Gcb802s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/Housing.csv')"
      ],
      "metadata": {
        "id": "v11ab4r_9Izw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "# Handling missing values\n",
        "# Data Preprocessing\n",
        "# Handling missing values\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "for col in df.select_dtypes(include=np.number): # Iterate over numeric columns only\n",
        "  df[col].fillna(df[col].median(), inplace=True) # Fill missing values with median of each numeric column\n",
        "\n",
        "\n",
        "# Defining features and target variable\n",
        "X = df.drop(columns=['price'])  # Assuming 'price' is the target variable\n",
        "y = df['price']\n",
        "\n",
        "# Splitting into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature engineering\n",
        "# Selecting numerical and categorical columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Preprocessing pipeline for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Creating a pipeline that first preprocesses the data, then applies a model\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "\n",
        "# Model training\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "y_pred = model_pipeline.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "print(f'Root Mean Squared Error: {rmse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N9RevCV9iTq",
        "outputId": "a4ff74da-7bbc-41ab-cf94-4fb9afbbce75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error: 1324506.9600914402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPAM EMAIL DETECTION\n"
      ],
      "metadata": {
        "id": "ktoAJ8VHAb12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/spam.csv', encoding='latin-1')  # Adjust encoding if needed\n",
        "\n",
        "# Preprocessing: use only the 'Category' and 'Message' columns\n",
        "data = data[['Category', 'Message']]\n",
        "\n",
        "# Convert 'Category' to binary (spam=1, ham=0)\n",
        "data['Category'] = data['Category'].map({'spam': 1, 'ham': 0})\n",
        "\n",
        "# Text vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(data['Message'])\n",
        "y = data['Category']\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_lr = log_reg.predict(X_test)\n",
        "print(\"Logistic Regression Accuracy: \", accuracy_score(y_test, y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "print(\"Random Forest Accuracy: \", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# AdaBoost Classifier\n",
        "ada_clf = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)\n",
        "y_pred_ada = ada_clf.predict(X_test)\n",
        "print(\"AdaBoost Accuracy: \", accuracy_score(y_test, y_pred_ada))\n",
        "print(classification_report(y_test, y_pred_ada))\n",
        "\n",
        "# K-Nearest Neighbors Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "y_pred_knn = knn_clf.predict(X_test)\n",
        "print(\"KNN Accuracy: \", accuracy_score(y_test, y_pred_knn))\n",
        "print(classification_report(y_test, y_pred_knn))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyAp_HkA9laJ",
        "outputId": "2ae2ed89-42e9-40f5-e9d8-651017708c1e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy:  0.9560538116591928\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98       966\n",
            "           1       1.00      0.67      0.80       149\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.98      0.84      0.89      1115\n",
            "weighted avg       0.96      0.96      0.95      1115\n",
            "\n",
            "Random Forest Accuracy:  0.9802690582959641\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       966\n",
            "           1       1.00      0.85      0.92       149\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.99      0.93      0.95      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "AdaBoost Accuracy:  0.9739910313901345\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99       966\n",
            "           1       0.94      0.86      0.90       149\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.96      0.93      0.94      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n",
            "KNN Accuracy:  0.9103139013452914\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95       966\n",
            "           1       1.00      0.33      0.49       149\n",
            "\n",
            "    accuracy                           0.91      1115\n",
            "   macro avg       0.95      0.66      0.72      1115\n",
            "weighted avg       0.92      0.91      0.89      1115\n",
            "\n"
          ]
        }
      ]
    }
  ]
}